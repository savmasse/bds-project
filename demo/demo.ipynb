{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================================\n",
    "# ======== Big Data Science: Project demo (May 2020) ========\n",
    "# ====================================================\n",
    "## Study of COVID-19 denialism through twitter activity\n",
    "### Project group 26:  Wannes Van Leemput, Sam Vanmassenhove\n",
    "\n",
    "In this project we study the phenomenon of \"COVID-denialism\", meaning people who, despite all evidence to the contrary, refuse to acknowledge the coronavirus as a serious threat to society, often referring to it as a \"hoax\". \n",
    "\n",
    "We searched corona-related tweets for signs of denialism and created a labelled training set based on the hashtags used by such denialists. The training data was then used to train a classification model in order to predict whether a certain tweet was a case of COVID denial or a regular tweet on the subject of coronavirus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../python files')\n",
    "\n",
    "# Import our own code\n",
    "from Authentication import Authentication\n",
    "from DataMiner import DataMiner\n",
    "from PreProcessTweets import PreProcessTweets\n",
    "from TweetDataIO import TweetDataIO\n",
    "from DenialPredictor import DenialPredictor\n",
    "from datastream_test import MyStreamListener\n",
    "from Visualisation import Visualisation\n",
    "from LocationService import LocationService\n",
    "\n",
    "# Create a set of English stopwords\n",
    "sw = set(stopwords.words(\"english\")) \n",
    "\n",
    "# Initiate spark\n",
    "#sc = SparkContext('local[*]')\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Get twitter api authentication\n",
    "api = Authentication().get_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification of denialist tweets\n",
    "### 1.1 Demonstration of tweet searching and IO\n",
    "\n",
    "We use the Tweepy library to search tweets. We focus on English language tweets as these are most common and more geopgraphically diverse.\n",
    "\n",
    "Note that the training set should already exist as a saved CSV-file when running this \"application\" in production. The following code is only for the purpose of demonstrating the IO functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tag: #BillGatesIsEvil\n",
      "Processing tag: #coronaHoax\n",
      "Processing tag: #FilmYourHospital\n",
      "Processing tag: #scamdemic\n",
      "Processing tag: #Plandemic2020\n",
      "Processing tag: #POTUS\n",
      "Processing tag: #QAnon\n",
      "Processing tag: #ResistTheNewWorldOrder\n",
      "Processing tag: #BillGates\n",
      "Processing tag: #CORONAHOAX\n",
      "Processing tag: #plandemic\n",
      "Processing tag: #Coronabollocks\n",
      "Processing tag: #sos\n",
      "Processing tag: #WWG1WGA\n",
      "Processing tag: #coronabollocks\n",
      "Processing tag: #Scamdemic\n",
      "Processing tag: #NWO\n",
      "Processing tag: #CovidHoax\n",
      "Processing tag: #q\n",
      "Processing tag: #woke\n",
      "Processing tag: #thegreatawakening\n",
      "Processing tag: #DrainTheSwamp\n",
      "Processing tag: #Coronahoax\n",
      "Processing tag: #BillGatesBioTerrorist\n",
      "Processing tag: #endthelockdown\n",
      "Processing tag: #FakePandemic\n",
      "Processing tag: #ObamaGate\n",
      "Processing tag: #Plandemic\n",
      "Processing tag: #coronahoax\n",
      "Processing tag: #CoronaHoax\n"
     ]
    }
   ],
   "source": [
    "# Mine some denial tweets (no specific location)\n",
    "tagignore = [\"#Covid_19\", \"#coronavirus\", \"#COVID„Éº19\", \"#COVID19\", \"#coronavirusNYC\", \"#coronavirusoregon\", \n",
    "             \"#lockdown\", \"#covid19\", \"#COVID\", \"#pandemic\", \"#Corona\", \"#Covid19\", \"#CoronaVirus\"]\n",
    "miner = DataMiner(api, \"#CoronaHoax\", \"\", \"en\", tagignore=tagignore, num_tweets=10)\n",
    "denial_tweets = miner.mine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tag: #Covid_19\n",
      "Processing tag: #Coronavirus\n",
      "Processing tag: #COVID19\n",
      "Processing tag: #coronavirus\n"
     ]
    }
   ],
   "source": [
    "# Mine some control tweets (no specific location)\n",
    "tagignore = [\"#CoronaHoax\", \"#covidhoax\",\"#coronahoax\", \"#covidhoax\", \"#Plandemic\"]\n",
    "miner = DataMiner(api, \"coronavirus\", \"\", \"en\", tagignore=tagignore, num_tweets=10)\n",
    "control_tweets = miner.mine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write the tweets to a CSV file\n",
    "filename = \"./training_data.txt\"\n",
    "io = TweetDataIO(filename, spark=spark, context=sc)\n",
    "io.write(denial_tweets, label=0, append=False)\n",
    "io.write(control_tweets, label=1, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+-------------------+-------------------+---------------+\n",
      "|label|            location|                tags|                text|               time|           tweet_id|           user|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+-------------------+---------------+\n",
      "|    0|    Salford, England|#BillGates|#BillG...|@BillGates #BillG...|2020-05-20 19:19:03|1263187500786466816|   SlickTrick14|\n",
      "|    0|                    |#Corruption|#Bill...|@BillGates Hey mo...|2020-05-20 19:09:20|1263185053963730946|       RikiCrew|\n",
      "|    0|A Wee Spot In Europe|#BillGatesVirus|#...|Both are meeting ...|2020-05-20 19:04:33|1263183848956977152|the_trading_ark|\n",
      "|    0|         Chicago, IL|#hr6666|#hr6666tr...|Is Contact Tracin...|2020-05-20 19:00:14|1263182764934823937|  anarkistbeatz|\n",
      "|    0|      United Kingdom|    #BillGatesIsEvil|@davidicke Agreed...|2020-05-20 18:55:40|1263181616731639809|      GatesAnti|\n",
      "|    0|                    |#BillGatesIsEvil|...|@realDonaldTrump ...|2020-05-20 18:07:50|1263169578823114752| OppressedHindu|\n",
      "|    0|                    |#BillGatesIsEvil|...|@shreena74052483 ...|2020-05-20 18:05:07|1263168895004770304| OppressedHindu|\n",
      "|    0|                    |    #BillGatesIsEvil|@shreena74052483 ...|2020-05-20 17:46:22|1263164173644468224| OppressedHindu|\n",
      "|    0|                    |#ChineseVirus|#Ch...|@ChristieGrunwa1 ...|2020-05-20 17:34:58|1263161305306890244|       rcoiteux|\n",
      "|    0|       New York, USA|him....#BillGates...|@EM_KA_17 This is...|2020-05-20 17:30:01|1263160060160749570|alexandrejakins|\n",
      "+-----+--------------------+--------------------+--------------------+-------------------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read same file and show\n",
    "ddf = io.read()\n",
    "ddf.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+-------------------+-------------------+---------------+\n",
      "|label|          location|                tags|                text|               time|           tweet_id|           user|\n",
      "+-----+------------------+--------------------+--------------------+-------------------+-------------------+---------------+\n",
      "|    0|   Alberta, Canada|#ObamaGate|#WWG1W...|Correct me if I‚Äôm...|2020-05-20 19:12:39|1263185890823847940|    ABPolitical|\n",
      "|    1|Khyber Pakhtunkhwa|#StayAtHome|#Stay...|Great #StayAtHome...|2020-05-20 19:20:57|1263187976147947522|     drishaq786|\n",
      "|    0|  Aichi-ken, Japan|#vote|#StillIVote...|Although not all ...|2020-05-20 09:56:19|1263045881864974336|     TheMusicks|\n",
      "|    0|                  |#COVID19|#Plandem...|This is just gros...|2020-05-20 18:15:37|1263171534883098625|    AmRedPilled|\n",
      "|    0|Massachusetts, USA|                  #Q|Thanks for linkin...|2020-05-20 19:14:47|1263186424888778756|featherjourney4|\n",
      "|    0|     United States|#WeThePeopleHaveH...|Everyone stop wha...|2020-05-20 19:18:16|1263187302588780553|  CaptNoahBuddy|\n",
      "|    0|                  |#ChristAnon|#Poli...|The History of th...|2020-05-20 19:04:53|1263183933375684609|   RealC_Knight|\n",
      "|    0|                  |#deepstatepuppet....|@bennyjohnson @mo...|2020-05-20 18:26:05|1263174170856378369|    woken_sheep|\n",
      "|    0|       Kearney, MO|                  #Q|@andreamoede At l...|2020-05-20 19:18:44|1263187420721360902|   MimiandGrump|\n",
      "|    0|     Greenwood, SC|#ResignNow!|#Make...|@SusanWojcicki @Y...|2020-05-20 18:13:18|1263170953800007683|  SpencerKarter|\n",
      "+-----+------------------+--------------------+--------------------+-------------------+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates \n",
    "ddf = ddf.orderBy(\"label\").dropDuplicates([\"tweet_id\"])\n",
    "ddf.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Perform preprocessing steps on the tweets to prepare for classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      ">> Removing stopwords...\n",
      ">> Removing urls...\n",
      ">> Removing hashtags...\n",
      ">> Removing user mentions...\n",
      ">> Removing punctuation...\n",
      ">> Removing whitespace...\n",
      "Finished preprocessing!\n"
     ]
    }
   ],
   "source": [
    "p = PreProcessTweets(ddf, \n",
    "                     remove_tags=True, \n",
    "                     remove_mentions=True, \n",
    "                     remove_punctuation=True, \n",
    "                     remove_urls=True, \n",
    "                     remove_stopwords=True)\n",
    "ddf = p.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>location</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Alberta, Canada</td>\n",
       "      <td>#ObamaGate|#WWG1WGA_WORLDWIDE|#WWG1WGA|#TheGre...</td>\n",
       "      <td>Correct me if I‚Äôm wrong but isn‚Äôt ‚Äúconspiracy ...</td>\n",
       "      <td>2020-05-20 19:12:39</td>\n",
       "      <td>1263185890823847940</td>\n",
       "      <td>ABPolitical</td>\n",
       "      <td>Correct I‚Äôm wrong isn‚Äôt ‚Äúconspiracy overthrow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Khyber Pakhtunkhwa</td>\n",
       "      <td>#StayAtHome|#StaySafe|#Covid_19</td>\n",
       "      <td>Great #StayAtHome #StaySafe #Covid_19 https://...</td>\n",
       "      <td>2020-05-20 19:20:57</td>\n",
       "      <td>1263187976147947522</td>\n",
       "      <td>drishaq786</td>\n",
       "      <td>Great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Aichi-ken, Japan</td>\n",
       "      <td>#vote|#StillIVote|#ResistTheNewWorldOrder|#Rev...</td>\n",
       "      <td>Although not all of problems can be solved wit...</td>\n",
       "      <td>2020-05-20 09:56:19</td>\n",
       "      <td>1263045881864974336</td>\n",
       "      <td>TheMusicks</td>\n",
       "      <td>Although problems solved Representation matter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>#COVID19|#Plandemic|#CovidHoax</td>\n",
       "      <td>This is just gross #COVID19 #Plandemic #CovidH...</td>\n",
       "      <td>2020-05-20 18:15:37</td>\n",
       "      <td>1263171534883098625</td>\n",
       "      <td>AmRedPilled</td>\n",
       "      <td>This gross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Massachusetts, USA</td>\n",
       "      <td>#Q</td>\n",
       "      <td>Thanks for linking to this article #Q ‚Äúmost co...</td>\n",
       "      <td>2020-05-20 19:14:47</td>\n",
       "      <td>1263186424888778756</td>\n",
       "      <td>featherjourney4</td>\n",
       "      <td>Thanks linking article ‚Äúmost committee work pu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label            location  \\\n",
       "0      0     Alberta, Canada   \n",
       "1      1  Khyber Pakhtunkhwa   \n",
       "2      0    Aichi-ken, Japan   \n",
       "3      0                       \n",
       "4      0  Massachusetts, USA   \n",
       "\n",
       "                                                tags  \\\n",
       "0  #ObamaGate|#WWG1WGA_WORLDWIDE|#WWG1WGA|#TheGre...   \n",
       "1                    #StayAtHome|#StaySafe|#Covid_19   \n",
       "2  #vote|#StillIVote|#ResistTheNewWorldOrder|#Rev...   \n",
       "3                     #COVID19|#Plandemic|#CovidHoax   \n",
       "4                                                 #Q   \n",
       "\n",
       "                                                text                 time  \\\n",
       "0  Correct me if I‚Äôm wrong but isn‚Äôt ‚Äúconspiracy ...  2020-05-20 19:12:39   \n",
       "1  Great #StayAtHome #StaySafe #Covid_19 https://...  2020-05-20 19:20:57   \n",
       "2  Although not all of problems can be solved wit...  2020-05-20 09:56:19   \n",
       "3  This is just gross #COVID19 #Plandemic #CovidH...  2020-05-20 18:15:37   \n",
       "4  Thanks for linking to this article #Q ‚Äúmost co...  2020-05-20 19:14:47   \n",
       "\n",
       "              tweet_id             user  \\\n",
       "0  1263185890823847940      ABPolitical   \n",
       "1  1263187976147947522       drishaq786   \n",
       "2  1263045881864974336       TheMusicks   \n",
       "3  1263171534883098625      AmRedPilled   \n",
       "4  1263186424888778756  featherjourney4   \n",
       "\n",
       "                                      processed_text  \n",
       "0  Correct I‚Äôm wrong isn‚Äôt ‚Äúconspiracy overthrow ...  \n",
       "1                                              Great  \n",
       "2  Although problems solved Representation matter...  \n",
       "3                                         This gross  \n",
       "4  Thanks linking article ‚Äúmost committee work pu...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to pandas and have a look at the new data\n",
    "df = ddf.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training and evaluation of classification model\n",
    "#### 1.3.1 First try on the original unedited text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set performance : \n",
      "Accuracy: 0.892,\n",
      "Precision: 1.000, \n",
      "Recall: 0.100,\n",
      "F1: 0.182\n",
      "Test set performance : \n",
      "Accuracy: 0.903,\n",
      "Precision: 1.000, \n",
      "Recall: 0.125,\n",
      "F1: 0.222\n"
     ]
    }
   ],
   "source": [
    "# Split data (corpus and labels) into train and test sets\n",
    "predictor = DenialPredictor(corpus=df.processed_text, labels=df.label, clf=\"nb\")\n",
    "X_train, X_test, y_train, y_test = predictor.train_test_split(split=0.3)\n",
    "\n",
    "# Fit the model\n",
    "predictor.fit_model(X_train, y_train)\n",
    "\n",
    "# Calculate some metrics to evaluate performance\n",
    "print(\"Training set performance : \")\n",
    "predictor.calc_metrics(X_train, y_train)\n",
    "print(\"Test set performance : \")\n",
    "predictor.calc_metrics(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Now do the same but without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      ">> Removing stopwords...\n",
      ">> Removing hashtags...\n",
      ">> Removing whitespace...\n",
      "Finished preprocessing!\n",
      "\n",
      "\n",
      "Training set performance : \n",
      "Accuracy: 0.886,\n",
      "Precision: 1.000, \n",
      "Recall: 0.050,\n",
      "F1: 0.095\n",
      "Test set performance : \n",
      "Accuracy: 0.889,\n",
      "Precision: 0.000, \n",
      "Recall: 0.000,\n",
      "F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "p = PreProcessTweets(ddf, \n",
    "                     remove_tags=True, \n",
    "                     remove_mentions=False, \n",
    "                     remove_punctuation=False, \n",
    "                     remove_urls=False, \n",
    "                     remove_stopwords=True)\n",
    "ddf = p.preprocess()\n",
    "df = ddf.toPandas()\n",
    "print(\"\\n\")\n",
    "\n",
    "# Split data (corpus and labels) into train and test sets\n",
    "predictor = DenialPredictor(df.processed_text, df.label)\n",
    "X_train, X_test, y_train, y_test = predictor.train_test_split(split=0.3)\n",
    "\n",
    "# Fit the model\n",
    "predictor.fit_model(X_train, y_train)\n",
    "\n",
    "# Calculate some metrics to evaluate performance\n",
    "print(\"Training set performance : \")\n",
    "predictor.calc_metrics(X_train, y_train)\n",
    "print(\"Test set performance : \")\n",
    "predictor.calc_metrics(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 The same again but this time with a different classifier: Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set performance : \n",
      "Accuracy: 1.000,\n",
      "Precision: 1.000, \n",
      "Recall: 1.000,\n",
      "F1: 1.000\n",
      "Test set performance : \n",
      "Accuracy: 0.903,\n",
      "Precision: 1.000, \n",
      "Recall: 0.125,\n",
      "F1: 0.222\n"
     ]
    }
   ],
   "source": [
    "# Split data (corpus and labels) into train and test sets\n",
    "svm_predictor = DenialPredictor(df.processed_text, df.label, clf=\"svm\")\n",
    "X_train, X_test, y_train, y_test = predictor.train_test_split(split=0.3)\n",
    "\n",
    "# Fit the model\n",
    "svm_predictor.fit_model(X_train, y_train)\n",
    "\n",
    "# Calculate some metrics to evaluate performance\n",
    "print(\"Training set performance : \")\n",
    "svm_predictor.calc_metrics(X_train, y_train)\n",
    "print(\"Test set performance : \")\n",
    "svm_predictor.calc_metrics(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Live tweet streaming and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LivePredictionStream import LivePredictionStream\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start streaming...\n",
      "(1) denial (0.94): How about another round of PPP\n",
      "(2) denial (0.93): Drop by our Facebook page or follow the link below to find out more about how schools in Clackmannanshire are consulting with parents and carers on their child‚Äôs wellbeing and learning ‚Å¶AtLochies‚Å©\n",
      "(3) denial (0.93): Everyday we have to ProtectTheVote\n",
      "(4) denial (0.89): üî•Check out this news coronavirus COVID19 COVID19 COVID„Éº19 Wuhan coronaviruschina WuhanCoronavirus ChinaVirus gtshare and RT\n",
      "(5) denial (0.93): Stanford has new free journalism coronavirus webinars DataDriven Storytelling Data Viz Be a Good Boss in Trying Times Innovate FastBetter Cyber Security\n",
      "(6) denial (0.93): The Trump administration gave a drugmaking contract worth up to 812 million to a small Virginia firm founded less than 6 months ago SmartNews\n",
      "(7) denial (0.96): Coronavirus of course they warn against it Because disagreeing with Trump is priority instead of saving lives Trump is withholding funding to the Who This is disgusting behavior by healthcare organizations that are clearly lying\n",
      "(8) denial (0.92): CNBC IBIO will be the major manufacturer of someone‚Äôs vaccine coronavirus MRNA INO PFE GILD\n",
      "(9) denial (0.91): Eric Trump claims coronavirus is Democratic hoax will ‚Äòmagically‚Äô vanish after 2020 election\n",
      "(10) denial (0.96): Coronavirus diamonds you can catch the flu\n",
      "(11) denial (0.96): I want Joe and Donald to debate headtohead with no teleprompters and no ear pieces I want Joe up there on his own defending his record and I want real questions not softballs thrown by liberal journalists from any network\n",
      "(12) denial (0.93): Coronavirus ¬£150m to be released from dormant accounts for charities and social enterprises\n",
      "(13) denial (0.91): Also FloridaVeteran1 Visit for all the details\n",
      "(14) denial (0.88): heartbreaking\n",
      "(15) denial (0.84): sweden top mortality covid19\n",
      "(16) denial (0.94): CDC now says coronavirus does not spread easily via contaminated surfaces Fox News\n",
      "(17) denial (0.88): protrump doctors\n",
      "(18) denial (0.89): Ford plant temporarily shuts day after reopening when workers test positive for coronavirus\n",
      "(19) denial (0.94): scrollin so you do agree Muslim men and coronavirus virus are same\n",
      "(20) denial (0.95): Trump thanks NationalGuard troops who have done such a good job amp risked their lives in the COVIDcrisis by making sure their service is limited to 89 days which means they dont get any pension or educational benefits under the GI Bill THANK YOU FOR YOUR SERVICE\n",
      "Stopped.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME = \"hoax.csv\"\n",
    "api = Authentication(isApp=False).get_api()\n",
    "auth = api.auth\n",
    "streamListener = LivePredictionStream(FILE_NAME, predictor, num_iter=20, verbose=1)\n",
    "stream = tweepy.Stream(auth=auth, listener=streamListener)\n",
    "\n",
    "try:\n",
    "    print('Start streaming...')\n",
    "    stream.filter(languages=['en'], \n",
    "                    track=[\"coronavirus\"])\n",
    "\n",
    "except Exception:\n",
    "    print(\"Stopped.\")\n",
    "\n",
    "finally:\n",
    "    print('Done.')\n",
    "    stream.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Aquiring tweet data\n",
    "\n",
    "Tweet data is aquired and preprocessed. Preprocessed includes geocoding the author location. Since a free api is used, we are limited to 1 request per second. Due to this, the geocoding can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationservice = LocationService()\n",
    "filename = \"./training_data.txt\"\n",
    "io = TweetDataIO(filename, spark=spark, context=sc)\n",
    "ddf = io.read()\n",
    "df = ddf.toPandas()\n",
    "df = locationservice.add_location_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise global trending hashtags\n",
    "Global trending hashtags amongst hoax-believer tweets are shown, along with their occurance count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"New York City\"\n",
    "country = \"USA\"\n",
    "radius = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-be00143521d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrending_tags_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vis' is not defined"
     ]
    }
   ],
   "source": [
    "vis.trending_tags_local(city, country, radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise hoax believers\n",
    "The location of hoax believers can be shown on a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.heat_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
