{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Big Data Science (May 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wannesvanleemput/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the Dutch stop words from the NLTK repository.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#\"44.4415,-102.6855,1000km\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer keys and access tokens, used for OAuth\n",
    "consumer_key = \"H2oOuvgoFBQ4PA1K9Yd8CqdM6\"        \n",
    "consumer_secret = \"zSa6ulXVdNAl1Xk6TMSw48nVXIm88suBF06JzmT5XNSG2AIBxH\"    \n",
    "access_token = \"1245776529768034306-YDUC9vTttvxvyhDhRVxGjfbt01p3bd\"          \n",
    "access_token_secret = \"7cDHZiGpHSD2Y8Fe6RdRIpe75WephsSmfU6woDhHlD5BX\"\n",
    "\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "#auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "#auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Calling the api: we want a lot of data in one run so we wait for the limit rather than run into error...\n",
    "api = tweepy.API(\n",
    "                auth,\n",
    "                wait_on_rate_limit=True,\n",
    "                wait_on_rate_limit_notify=True\n",
    "                )\n",
    "if not api:\n",
    "    print(\"Can't authenticate. Check if credentials are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashtagFinder():\n",
    "    \n",
    "    def __init__(self, starting_hashtag, location_radius, language, tagignore):\n",
    "        self.starting_hashtag = starting_hashtag\n",
    "        self.location_radius = location_radius\n",
    "        self.language = language\n",
    "        self.hashtags = {}\n",
    "        self.tagignore = tagignore\n",
    "        \n",
    "    def collect_tags(self, tag=\"\"):\n",
    "        if tag == \"\":\n",
    "            tag = self.starting_hashtag\n",
    "        items = tweepy.Cursor(api.search,\n",
    "                             q = tag + \" -filter:retweets\",\n",
    "                             geocode=self.location_radius,\n",
    "                             count=100,\n",
    "                             lang=self.language,\n",
    "                             include_rts=False,\n",
    "                             tweet_mode=\"extended\").items(1000)\n",
    "        items = list(items)\n",
    "        tweets = [t.full_text for t in items]\n",
    "        for index, tweet in enumerate(tweets):\n",
    "            tags = self.hashtags_from_tweet(tweet)\n",
    "            for tag in tags:\n",
    "                if tag not in self.tagignore:\n",
    "                    if tag in self.hashtags:\n",
    "                        self.hashtags[tag] = self.hashtags[tag] + 1\n",
    "                    else:\n",
    "                        self.hashtags[tag] = 1\n",
    "        \n",
    "    def get_hashtags(self):\n",
    "        return self.hashtags\n",
    "    \n",
    "    def hashtags_from_tweet(self, tweet):\n",
    "        words = tweet.split()\n",
    "        tags = [\"#\"+re.sub(r'\\W+','', word) for word in words if \"#\" in word]\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Best to do data mining with specific class once program becomes more complex.\n",
    "\"\"\"\n",
    "class DataMiner():\n",
    "    \n",
    "    def __init__(self, starting_hashtag, location_radius, language, tagignore = []):\n",
    "        self.location_radius = location_radius\n",
    "        self.language = language\n",
    "        self.denial_tweets = []\n",
    "        self.ids = []\n",
    "        self.finder = HashtagFinder(starting_hashtag, location_radius, language, tagignore)\n",
    "        \n",
    "    def _collect_tweets(self):\n",
    "        #find relevant hashtags to search for\n",
    "        self.finder.collect_tags()\n",
    "        denial_tags = self.finder.get_hashtags()\n",
    "        denial_tags = {k: v for k, v in sorted(denial_tags.items(), key=lambda item: item[1]) if v >= 10}\n",
    "        \n",
    "        for k, v in denial_tags.items():\n",
    "            print(\"Processing tag: \" + k)\n",
    "            search_term = k + \" -filter:retweets\"\n",
    "            items = tweepy.Cursor(api.search,\n",
    "                                q = search_term,\n",
    "                                geocode=self.location_radius,\n",
    "                                count=100,\n",
    "                                lang=self.language,\n",
    "                                include_rts=False,\n",
    "                                tweet_mode=\"extended\").items(500)\n",
    "            items = list(items)\n",
    "            for item in items:\n",
    "                if item not in self.ids:\n",
    "                    self.ids.append(item.id)\n",
    "                    self.denial_tweets.append(item.full_text)\n",
    "                    \n",
    "    def mine(self):\n",
    "        self._collect_tweets()\n",
    "        return self.denial_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for the preprocessing of tweets; involves removing hyperlinks and stopwords.\n",
    "\"\"\"\n",
    "class PreProcessTweets():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 tweets, \n",
    "                 remove_tags=False, \n",
    "                 remove_stopwords=False, \n",
    "                 remove_urls=False,\n",
    "                 remove_mentions=False,\n",
    "                 remove_punctuation=False):\n",
    "        \n",
    "        self.tweets = tweets\n",
    "        self.remove_tags = remove_tags\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_mentions = remove_mentions\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        \n",
    "    def _remove_urls(self):\n",
    "        \"\"\" Remove all urls from the tweet text. \"\"\"\n",
    "        self.tweets = [re.sub(r'\\s?http\\S+', \"\", t) for t in self.tweets]\n",
    "    \n",
    "    def _remove_stopwords(self):\n",
    "        \"\"\" Remove English stopwords from the text. \"\"\"\n",
    "        sw = set(stopwords.words(\"english\")) \n",
    "        self.tweets = [\" \".join([word for word in c.split() if word not in sw]) for c in self.tweets]\n",
    "\n",
    "    def _remove_hashtag(self, tag=None):\n",
    "        \"\"\" Remove a specific hashtag. If no tag specified, remove all tags.\"\"\"\n",
    "        for index, tweet in enumerate(self.tweets):\n",
    "            words = tweet.split()\n",
    "            no_tags = [word for word in words if \"#\" not in word]\n",
    "            self.tweets[index] = \" \".join(no_tags)\n",
    "            \n",
    "    def _remove_mentions(self):\n",
    "        \"\"\" Remove all mentions (@user). \"\"\"\n",
    "        self.tweets = [re.sub(r'\\s?@\\S+', \"\", t) for t in self.tweets]\n",
    "    \n",
    "    def _remove_punctuation(self):\n",
    "        \"\"\" Punctuation affects words: eg. 'however' is not the same word as 'however,'\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self):\n",
    "        \"\"\" Perform the requested steps of the preprocessing. \"\"\"\n",
    "        \n",
    "        if self.remove_tags:\n",
    "            self._remove_hashtag()\n",
    "            \n",
    "        if self.remove_stopwords:\n",
    "            self._remove_stopwords()\n",
    "            \n",
    "        if self.remove_urls:\n",
    "            self._remove_urls()\n",
    "            \n",
    "        if self.remove_mentions:\n",
    "            self._remove_mentions()\n",
    "        \n",
    "        if self.remove_punctuation:\n",
    "            self._remove_punctuation()\n",
    "            \n",
    "        return self.tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet data mining\n",
    "\n",
    "We use the Tweepy.Cursor functionality to search for tweets on the coronavirus topic in a certain geographical area in the United States. We take New York City because this region has been most affected by the virus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TWEETS = 1000\n",
    "SEARCH_TERM = \"#CoronaHoax -filter:retweets\"\n",
    "location_radius = \"40.7282,-73.7949,1000km\"\n",
    "language = \"en\"\n",
    "starting_hashtag = \"#CoronaHoax\"\n",
    "#ignore generic tags\n",
    "tagignore = [\"#Covid_19\", \"#coronavirus\", \"#COVIDãƒ¼19\", \"#COVID19\", \"#coronavirusNYC\", \"#coronavirusoregon\", \"#lockdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tag: #COVIDIOTS\n",
      "Processing tag: #endthelockdown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 32\n"
     ]
    }
   ],
   "source": [
    "miner = DataMiner(starting_hashtag, location_radius, language, tagignore)\n",
    "denial_tweets = miner.mine()\n",
    "print(f\"Processed {len(list(denial_tweets))} tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"coronavirus -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(5000)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"covid -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(5000)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets.extend([t.full_text for t in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"lockdown -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(5000)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets.extend([t.full_text for t in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some preprocessing on the text\n",
    "tweets = denial_tweets + control_tweets\n",
    "preprocessor = PreProcessTweets(\n",
    "                                tweets.copy(), \n",
    "                                remove_tags=True,\n",
    "                                remove_urls=True,\n",
    "                                remove_stopwords=True,\n",
    "                                remove_mentions=True\n",
    "                               )\n",
    "corpus = preprocessor.preprocess()\n",
    "labels = [0]*len(denial_tweets) + [1]*len(control_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the input data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    labels, \n",
    "                                                    random_state=123,\n",
    "                                                    test_size=0.3\n",
    "                                                   )\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Get some performance metrics on the training set\n",
    "y_predict = model.predict(X_train)\n",
    "\n",
    "a = accuracy_score(y_train, y_predict)\n",
    "p = precision_score(y_train, y_predict)\n",
    "r = recall_score(y_train, y_predict)\n",
    "f = f1_score(y_train, y_predict)\n",
    "print(\"Training performance metrics: \")\n",
    "print(f\"\\t-Accuracy: {a:.3f},\\n\\t-Precision: {p:.3f}, \\n\\t-Recall: {r:.3f},\\n\\t-F1: {f:.3f}\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Get some performance metrics on the test set\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "a = accuracy_score(y_test, y_predict)\n",
    "p = precision_score(y_test, y_predict)\n",
    "r = recall_score(y_test, y_predict)\n",
    "f = f1_score(y_test, y_predict)\n",
    "print(\"Test performance metrics: \")\n",
    "print(f\"\\t-Accuracy: {a:.3f},\\n\\t-Precision: {p:.3f}, \\n\\t-Recall: {r:.3f},\\n\\t-F1: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world test\n",
    "\n",
    "We can download some more tweets from the same #CoronaHoax hashtag we started with and check that these are indeed flagged correctly as \"COVID denial\" tweets. The tweets now originated from Los Angeles, so there is no overlap from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"#CoronaHoax -filter:retweets\", \n",
    "            geocode=\"34.0522,-118.2436,500km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(100)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "tweets_LA = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorLA = PreProcessTweets(\n",
    "                                tweets_LA.copy(), \n",
    "                                remove_tags=True,\n",
    "                                remove_urls=True,\n",
    "                                remove_stopwords=True,\n",
    "                                remove_mentions=True\n",
    "                               )\n",
    "corpus = preprocessorLA.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.transform(corpus)\n",
    "y_predict = model.predict(x)\n",
    "print(f\"Accuracy: {list(y_predict).count(0) / len(y_predict):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
