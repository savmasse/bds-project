{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Big Data Science (May 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SamVa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import 3rd party libraries\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our own code\n",
    "from DataMiner import DataMiner\n",
    "from HashtagFinder import HashtagFinder\n",
    "from PreProcessTweets import PreProcessTweets\n",
    "from Authentication import Authentication\n",
    "\n",
    "# Download the Dutch stop words from the NLTK repository.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#\"44.4415,-102.6855,1000km\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = Authentication()\n",
    "api = auth.get_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet data mining\n",
    "\n",
    "We use the Tweepy.Cursor functionality to search for tweets on the coronavirus topic in a certain geographical area in the United States. We take New York City because this region has been most affected by the virus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TWEETS = 5000\n",
    "SEARCH_TERM = \"#CoronaHoax -filter:retweets\"\n",
    "location_radius = \"40.7282,-73.7949,1000km\"\n",
    "language = \"en\"\n",
    "starting_hashtag = \"#CoronaHoax\"\n",
    "#ignore generic tags, since they could corrupt the denial-tweets dataset\n",
    "tagignore = [\"#Covid_19\", \"#coronavirus\", \"#COVIDãƒ¼19\", \"#COVID19\", \"#coronavirusNYC\", \"#coronavirusoregon\", \"#lockdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tag: #COVID\n",
      "Processing tag: #endthelockdown\n",
      "Processing tag: #COVIDIOT\n",
      "Processing tag: #COVIDIOTS\n",
      "Processing tag: #CoronavirusLiar\n",
      "Processing tag: #scamdemic\n",
      "Processing tag: #FakePandemic\n",
      "Processing tag: #ObamaGate\n",
      "Processing tag: #KateTheFascist\n",
      "Processing tag: #Coronahoax\n",
      "Processing tag: #KateNazi\n",
      "Processing tag: #LoveTrumpsKate\n",
      "Processing tag: #LockUpKate\n",
      "Processing tag: #NoMoreKate\n",
      "Processing tag: #Plandemic\n",
      "Processing tag: #reopenNYC\n",
      "Processing tag: #coronapanic\n",
      "Processing tag: #ReopenOregon\n",
      "Processing tag: #coronavirustruth\n",
      "Processing tag: #ReopenAmerica\n",
      "Processing tag: #coronahoax\n",
      "Processing tag: #Qannon\n",
      "Processing tag: #QAnon2020\n",
      "Processing tag: #Trump2020Landslide\n",
      "Processing tag: #QAnons\n",
      "Processing tag: #Trump2020NowMoreThanEver\n",
      "Processing tag: #trump\n",
      "Processing tag: #MAGA2020\n",
      "Processing tag: #maga\n",
      "Processing tag: #Trump2020\n",
      "Processing tag: #CoronaHoax\n",
      "Processed 10139 tweets.\n"
     ]
    }
   ],
   "source": [
    "miner = DataMiner(api, starting_hashtag, location_radius, language, tagignore)\n",
    "denial_tweets = miner.mine()\n",
    "print(f\"Processed {len(list(denial_tweets))} tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 5000 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"coronavirus -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(NUM_TWEETS)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 5000 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"covid -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(NUM_TWEETS)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets.extend([t.full_text for t in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 5000 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"lockdown -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(NUM_TWEETS)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets.extend([t.full_text for t in items])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some preprocessing on the text\n",
    "tweets = denial_tweets + control_tweets\n",
    "preprocessor = PreProcessTweets(\n",
    "                                tweets.copy(),\n",
    "                                remove_tags=False,\n",
    "                                remove_urls=True,\n",
    "                                remove_stopwords=True,\n",
    "                                remove_mentions=True\n",
    "                               )\n",
    "corpus = preprocessor.preprocess()\n",
    "labels = [0]*len(denial_tweets) + [1]*len(control_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance metrics: \n",
      "\t-Accuracy: 0.899,\n",
      "\t-Precision: 0.905, \n",
      "\t-Recall: 0.927,\n",
      "\t-F1: 0.916\n",
      "===================================\n",
      "Test performance metrics: \n",
      "\t-Accuracy: 0.832,\n",
      "\t-Precision: 0.858, \n",
      "\t-Recall: 0.866,\n",
      "\t-F1: 0.862\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the input data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    labels, \n",
    "                                                    random_state=123,\n",
    "                                                    test_size=0.3\n",
    "                                                   )\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Get some performance metrics on the training set\n",
    "y_predict = model.predict(X_train)\n",
    "\n",
    "a = accuracy_score(y_train, y_predict)\n",
    "p = precision_score(y_train, y_predict)\n",
    "r = recall_score(y_train, y_predict)\n",
    "f = f1_score(y_train, y_predict)\n",
    "print(\"Training performance metrics: \")\n",
    "print(f\"\\t-Accuracy: {a:.3f},\\n\\t-Precision: {p:.3f}, \\n\\t-Recall: {r:.3f},\\n\\t-F1: {f:.3f}\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Get some performance metrics on the test set\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "a = accuracy_score(y_test, y_predict)\n",
    "p = precision_score(y_test, y_predict)\n",
    "r = recall_score(y_test, y_predict)\n",
    "f = f1_score(y_test, y_predict)\n",
    "print(\"Test performance metrics: \")\n",
    "print(f\"\\t-Accuracy: {a:.3f},\\n\\t-Precision: {p:.3f}, \\n\\t-Recall: {r:.3f},\\n\\t-F1: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world test\n",
    "\n",
    "We can download some more tweets from the same #CoronaHoax hashtag we started with and check that these are indeed flagged correctly as \"COVID denial\" tweets. The tweets now originated from Los Angeles, so there is no overlap from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 100 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"#Plandemic -filter:retweets\", \n",
    "            geocode=\"34.0522,-118.2436,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(100)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "tweets_LA = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorLA = PreProcessTweets(\n",
    "                                tweets_LA.copy(), \n",
    "                                remove_tags=False,\n",
    "                                remove_urls=True,\n",
    "                                remove_stopwords=True,\n",
    "                                remove_mentions=True\n",
    "                               )\n",
    "corpus = preprocessorLA.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.910\n"
     ]
    }
   ],
   "source": [
    "x = vectorizer.transform(corpus)\n",
    "y_predict = model.predict(x)\n",
    "print(f\"Accuracy: {list(y_predict).count(0) / len(y_predict):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
