{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Big Data Science (May 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SamVa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tweepy\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the Dutch stop words from the NLTK repository.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#\"44.4415,-102.6855,1000km\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer keys and access tokens, used for OAuth\n",
    "consumer_key = \"H2oOuvgoFBQ4PA1K9Yd8CqdM6\"        \n",
    "consumer_secret = \"zSa6ulXVdNAl1Xk6TMSw48nVXIm88suBF06JzmT5XNSG2AIBxH\"    \n",
    "access_token = \"1245776529768034306-YDUC9vTttvxvyhDhRVxGjfbt01p3bd\"          \n",
    "access_token_secret = \"7cDHZiGpHSD2Y8Fe6RdRIpe75WephsSmfU6woDhHlD5BX\"\n",
    "\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "#auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "#auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Calling the api: we want a lot of data in one run so we wait for the limit rather than run into error...\n",
    "api = tweepy.API(\n",
    "                auth, \n",
    "                wait_on_rate_limit=True,\n",
    "                wait_on_rate_limit_notify=True\n",
    "                ) \n",
    "\n",
    "if not api:\n",
    "    print(\"Can't authenticate. Check if credentials are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Best to do data mining with specific class once program becomes more complex.\n",
    "\"\"\"\n",
    "class DataMiner():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for the preprocessing of tweets; involves removing hyperlinks and stopwords.\n",
    "\"\"\"\n",
    "class PreProcessTweets():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 tweets, \n",
    "                 remove_tags=False, \n",
    "                 remove_stopwords=False, \n",
    "                 remove_urls=False,\n",
    "                 remove_mentions=False,\n",
    "                 remove_punctuation=False):\n",
    "        \n",
    "        self.tweets = tweets\n",
    "        self.remove_tags = remove_tags\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_mentions = remove_mentions\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        \n",
    "    def _remove_urls(self):\n",
    "        \"\"\" Remove all urls from the tweet text. \"\"\"\n",
    "        self.tweets = [re.sub(r'\\s?http\\S+', \"\", t) for t in self.tweets]\n",
    "    \n",
    "    def _remove_stopwords(self):\n",
    "        \"\"\" Remove English stopwords from the text. \"\"\"\n",
    "        sw = set(stopwords.words(\"english\")) \n",
    "        self.tweets = [\" \".join([word for word in c.split() if word not in sw]) for c in self.tweets]\n",
    "\n",
    "    def _remove_hashtag(self, tag=None):\n",
    "        \"\"\" Remove a specific hashtag. If no tag specified, remove all tags.\"\"\"\n",
    "        for index, tweet in enumerate(self.tweets):\n",
    "            words = tweet.split()\n",
    "            no_tags = [word for word in words if \"#\" not in word]\n",
    "            self.tweets[index] = \" \".join(no_tags)\n",
    "            \n",
    "    def _remove_mentions(self):\n",
    "        \"\"\" Remove all mentions (@user). \"\"\"\n",
    "        self.tweets = [re.sub(r'\\s?@\\S+', \"\", t) for t in self.tweets]\n",
    "    \n",
    "    def _remove_punctuation(self):\n",
    "        \"\"\" Punctuation affects words: eg. 'however' is not the same word as 'however,'\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self):\n",
    "        \"\"\" Perform the requested steps of the preprocessing. \"\"\"\n",
    "        \n",
    "        if self.remove_tags:\n",
    "            self._remove_hashtag()\n",
    "            \n",
    "        if self.remove_stopwords:\n",
    "            self._remove_stopwords()\n",
    "            \n",
    "        if self.remove_urls:\n",
    "            self._remove_urls()\n",
    "            \n",
    "        if self.remove_mentions:\n",
    "            self._remove_mentions()\n",
    "        \n",
    "        if self.remove_punctuation:\n",
    "            self._remove_punctuation()\n",
    "            \n",
    "        return self.tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet data mining\n",
    "\n",
    "We use the Tweepy.Cursor functionality to search for tweets on the coronavirus topic in a certain geographical area in the United States. We take New York City because this region has been most affected by the virus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TWEETS = 1000\n",
    "SEARCH_TERM = \"#CoronaHoax -filter:retweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 572 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=SEARCH_TERM, \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(NUM_TWEETS)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "denial_tweets = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 1000 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"coronavirus -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,100km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(NUM_TWEETS)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some preprocessing on the text\n",
    "tweets = denial_tweets + control_tweets\n",
    "preprocessor = PreProcessTweets(\n",
    "                                tweets.copy(), \n",
    "                                remove_tags=True,\n",
    "                                remove_urls=True,\n",
    "                                remove_stopwords=True,\n",
    "                                remove_mentions=True\n",
    "                               )\n",
    "corpus = preprocessor.preprocess()\n",
    "labels = [0]*len(denial_tweets) + [1]*len(control_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance metrics: \n",
      "\t-Accuracy: 0.967,\n",
      "\t-Precision: 0.970, \n",
      "\t-Recall: 0.978,\n",
      "\t-F1: 0.974\n",
      "===================================\n",
      "Test performance metrics: \n",
      "\t-Accuracy: 0.765,\n",
      "\t-Precision: 0.842, \n",
      "\t-Recall: 0.796,\n",
      "\t-F1: 0.818\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the input data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    labels, \n",
    "                                                    random_state=123,\n",
    "                                                    test_size=0.3\n",
    "                                                   )\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Get some performance metrics on the training set\n",
    "y_predict = model.predict(X_train)\n",
    "\n",
    "a = accuracy_score(y_train, y_predict)\n",
    "p = precision_score(y_train, y_predict)\n",
    "r = recall_score(y_train, y_predict)\n",
    "f = f1_score(y_train, y_predict)\n",
    "print(\"Training performance metrics: \")\n",
    "print(f\"\\t-Accuracy: {a:.3f},\\n\\t-Precision: {p:.3f}, \\n\\t-Recall: {r:.3f},\\n\\t-F1: {f:.3f}\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Get some performance metrics on the test set\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "a = accuracy_score(y_test, y_predict)\n",
    "p = precision_score(y_test, y_predict)\n",
    "r = recall_score(y_test, y_predict)\n",
    "f = f1_score(y_test, y_predict)\n",
    "print(\"Test performance metrics: \")\n",
    "print(f\"\\t-Accuracy: {a:.3f},\\n\\t-Precision: {p:.3f}, \\n\\t-Recall: {r:.3f},\\n\\t-F1: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world test\n",
    "\n",
    "We can download some more tweets from another similar hashtag and check that these are indeed flagged correctly as \"COVID denial\" tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 55 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"#FakePandemic -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,100km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(100)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "tweets = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.727\n"
     ]
    }
   ],
   "source": [
    "x = vectorizer.transform(tweets)\n",
    "y_predict = model.predict(x)\n",
    "print(f\"Accuracy: {list(y_predict).count(0) / len(y_predict):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
