{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Big Data Science (May 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wannesvanleemput/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import 3rd party libraries\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import our own code\n",
    "from DataMiner import DataMiner\n",
    "from HashtagFinder import HashtagFinder\n",
    "from PreProcessTweets import PreProcessTweets\n",
    "from Authentication import Authentication\n",
    "from LocationService import LocationService\n",
    "\n",
    "# Download the Dutch stop words from the NLTK repository.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#\"44.4415,-102.6855,1000km\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = Authentication()\n",
    "api = auth.get_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet data mining\n",
    "\n",
    "We use the Tweepy.Cursor functionality to search for tweets on the coronavirus topic in a certain geographical area in the United States. We take New York City because this region has been most affected by the virus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TWEETS = 5000\n",
    "SEARCH_TERM = \"#CovidHoax -filter:retweets\"\n",
    "loc = LocationService()\n",
    "lat, lng = loc.get_coordinates(\"New York City\", \"United States\")\n",
    "location_radius = str(lat) + \",\" + str(lng) + \",100km\"\n",
    "language = \"en\"\n",
    "starting_hashtag = \"#Plandemic\"\n",
    "#ignore generic tags, since they could corrupt the denial-tweets dataset\n",
    "tagignore = [\"#Covid_19\", \"#coronavirus\", \"#COVIDãƒ¼19\", \"#COVID19\", \"#coronavirusNYC\", \"#coronavirusoregon\", \"#lockdown\", \"#covid19\", \"#COVID\", \"#pandemic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tag: #scamdemic\n",
      "Processing tag: #Plandemic2020\n",
      "Processing tag: #ObamaGate\n",
      "Processing tag: #PlandemicDocumentary\n",
      "Processing tag: #ConspiracyTheory\n",
      "Processing tag: #plandemic\n",
      "Processing tag: #Plandemic\n",
      "Processed 2160 tweets.\n"
     ]
    }
   ],
   "source": [
    "miner = DataMiner(api, starting_hashtag, location_radius, language, tagignore)\n",
    "denial_tweets = miner.mine()\n",
    "print(f\"Processed {len(list(denial_tweets))} tweets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tag: #COVID\n",
      "Processing tag: #fakenews\n",
      "Processing tag: #PLANdemic\n",
      "Processing tag: #pandemic\n",
      "Processing tag: #QAnon\n",
      "Processing tag: #WWG1GWA\n",
      "Processing tag: #DeepState\n",
      "Processing tag: #COVIDIOTS\n",
      "Processing tag: #Fauci\n",
      "Processing tag: #qanon\n",
      "Processing tag: #PLANDEMIC\n",
      "Processing tag: #FauciFraud\n",
      "Processing tag: #scamdemic\n",
      "Processing tag: #Plandemic2020\n",
      "Processing tag: #ObamaGate\n",
      "Processing tag: #PlandemicDocumentary\n",
      "Processing tag: #ConspiracyTheory\n",
      "Processing tag: #plandemic\n",
      "Processing tag: #Plandemic\n"
     ]
    }
   ],
   "source": [
    "denial_tweets.extend(miner.mine(\"#CoronaHoax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Location</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nestor Delgado</td>\n",
       "      <td>Brooklyn, New York</td>\n",
       "      <td>[#scamdemic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luna77</td>\n",
       "      <td>New Jersey, USA</td>\n",
       "      <td>[#Scamdemic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kat</td>\n",
       "      <td>Florida, USA</td>\n",
       "      <td>[#scamdemic, #controlaviris]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yoshi Yokamura</td>\n",
       "      <td>New Jersey, USA</td>\n",
       "      <td>[#pizzagate, #Scamdemic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yoshi Yokamura</td>\n",
       "      <td>New Jersey, USA</td>\n",
       "      <td>[#scamdemic]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Author             Location                          Tags\n",
       "0  Nestor Delgado   Brooklyn, New York                  [#scamdemic]\n",
       "1          Luna77      New Jersey, USA                  [#Scamdemic]\n",
       "2             Kat         Florida, USA  [#scamdemic, #controlaviris]\n",
       "3  Yoshi Yokamura      New Jersey, USA      [#pizzagate, #Scamdemic]\n",
       "4  Yoshi Yokamura      New Jersey, USA                  [#scamdemic]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda = miner.get_dataframe()\n",
    "panda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 5000 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"coronavirus -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(NUM_TWEETS)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"covid -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(NUM_TWEETS)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets.extend([t.full_text for t in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"lockdown -filter:retweets\", \n",
    "            geocode=\"40.7282,-73.7949,1000km\",\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(NUM_TWEETS)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "control_tweets.extend([t.full_text for t in items])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some preprocessing on the text\n",
    "tweets = denial_tweets + control_tweets\n",
    "preprocessor = PreProcessTweets(\n",
    "                                tweets.copy(),\n",
    "                                remove_tags=False,\n",
    "                                remove_urls=True,\n",
    "                                remove_stopwords=True,\n",
    "                                remove_mentions=True\n",
    "                               )\n",
    "corpus = preprocessor.preprocess()\n",
    "labels = [0]*len(denial_tweets) + [1]*len(control_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training performance metrics: \n",
      "\t-Accuracy: 0.975,\n",
      "\t-Precision: 0.978, \n",
      "\t-Recall: 0.983,\n",
      "\t-F1: 0.981\n",
      "===================================\n",
      "Test performance metrics: \n",
      "\t-Accuracy: 0.960,\n",
      "\t-Precision: 0.971, \n",
      "\t-Recall: 0.967,\n",
      "\t-F1: 0.969\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the input data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    labels, \n",
    "                                                    random_state=123,\n",
    "                                                    test_size=0.3\n",
    "                                                   )\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Get some performance metrics on the training set\n",
    "y_predict = model.predict(X_train)\n",
    "\n",
    "a = accuracy_score(y_train, y_predict)\n",
    "p = precision_score(y_train, y_predict)\n",
    "r = recall_score(y_train, y_predict)\n",
    "f = f1_score(y_train, y_predict)\n",
    "print(\"Training performance metrics: \")\n",
    "print(f\"\\t-Accuracy: {a:.3f},\\n\\t-Precision: {p:.3f}, \\n\\t-Recall: {r:.3f},\\n\\t-F1: {f:.3f}\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Get some performance metrics on the test set\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "a = accuracy_score(y_test, y_predict)\n",
    "p = precision_score(y_test, y_predict)\n",
    "r = recall_score(y_test, y_predict)\n",
    "f = f1_score(y_test, y_predict)\n",
    "print(\"Test performance metrics: \")\n",
    "print(f\"\\t-Accuracy: {a:.3f},\\n\\t-Precision: {p:.3f}, \\n\\t-Recall: {r:.3f},\\n\\t-F1: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world test\n",
    "\n",
    "We can download some more tweets from the same #CoronaHoax hashtag we started with and check that these are indeed flagged correctly as \"COVID denial\" tweets. The tweets now originated from Los Angeles, so there is no overlap from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = LocationService()\n",
    "lat, lng = loc.get_coordinates(\"Los Angeles\", \"United States\")\n",
    "location = str(lat) + \",\" + str(lng) + \",500km\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading 74 items.\n"
     ]
    }
   ],
   "source": [
    "items = tweepy.Cursor(api.search,\n",
    "            q=\"coronahoax -filter:retweets\", \n",
    "            geocode=location,\n",
    "            count=100,\n",
    "            lang=\"en\",\n",
    "            include_rts=False,\n",
    "            tweet_mode=\"extended\").items(100)\n",
    "\n",
    "items = list(items)\n",
    "print(f\"Finished reading {len(items)} items.\")\n",
    "tweets_LA = [t.full_text for t in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessorLA = PreProcessTweets(\n",
    "                                tweets_LA.copy(), \n",
    "                                remove_tags=False,\n",
    "                                remove_urls=True,\n",
    "                                remove_stopwords=True,\n",
    "                                remove_mentions=True\n",
    "                               )\n",
    "corpus = preprocessorLA.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.824\n"
     ]
    }
   ],
   "source": [
    "x = vectorizer.transform(corpus)\n",
    "y_predict = model.predict(x)\n",
    "print(f\"Accuracy: {list(y_predict).count(0) / len(y_predict):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
